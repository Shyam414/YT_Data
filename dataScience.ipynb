{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\sunda\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "from collections import Counter\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>videoId</th>\n",
       "      <th>title</th>\n",
       "      <th>videoLikes</th>\n",
       "      <th>videoViews</th>\n",
       "      <th>commenter_name</th>\n",
       "      <th>comments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Fj-g-cmSzbo</td>\n",
       "      <td>Beautiful but toxic.</td>\n",
       "      <td>56482</td>\n",
       "      <td>808475</td>\n",
       "      <td>@RohitandKanu</td>\n",
       "      <td>you guys know of any other beautiful but toxic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Fj-g-cmSzbo</td>\n",
       "      <td>Beautiful but toxic.</td>\n",
       "      <td>56482</td>\n",
       "      <td>808475</td>\n",
       "      <td>@rignudignu7200</td>\n",
       "      <td>a porsche</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Fj-g-cmSzbo</td>\n",
       "      <td>Beautiful but toxic.</td>\n",
       "      <td>56482</td>\n",
       "      <td>808475</td>\n",
       "      <td>@mastmogachi</td>\n",
       "      <td>monkey shoulder  is looking very beautiful rig...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Fj-g-cmSzbo</td>\n",
       "      <td>Beautiful but toxic.</td>\n",
       "      <td>56482</td>\n",
       "      <td>808475</td>\n",
       "      <td>@DD-pe3no</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Fj-g-cmSzbo</td>\n",
       "      <td>Beautiful but toxic.</td>\n",
       "      <td>56482</td>\n",
       "      <td>808475</td>\n",
       "      <td>@MrKamei</td>\n",
       "      <td>my crush</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6298</th>\n",
       "      <td>xxwNglca0Bo</td>\n",
       "      <td>Limca Book of World Records - Renu Sukheja</td>\n",
       "      <td>1073</td>\n",
       "      <td>149466</td>\n",
       "      <td>@dmcindianfan5374</td>\n",
       "      <td>just went back 10 years ago</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6299</th>\n",
       "      <td>xxwNglca0Bo</td>\n",
       "      <td>Limca Book of World Records - Renu Sukheja</td>\n",
       "      <td>1073</td>\n",
       "      <td>149466</td>\n",
       "      <td>@coolbreeze1262</td>\n",
       "      <td>is the record still to her</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6300</th>\n",
       "      <td>xxwNglca0Bo</td>\n",
       "      <td>Limca Book of World Records - Renu Sukheja</td>\n",
       "      <td>1073</td>\n",
       "      <td>149466</td>\n",
       "      <td>@Notebook-ur1st</td>\n",
       "      <td>wow she looks different  \\ni follow them for e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6301</th>\n",
       "      <td>xxwNglca0Bo</td>\n",
       "      <td>Limca Book of World Records - Renu Sukheja</td>\n",
       "      <td>1073</td>\n",
       "      <td>149466</td>\n",
       "      <td>@Prathikraj_mysorean</td>\n",
       "      <td>one can see rohits journey</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6302</th>\n",
       "      <td>xxwNglca0Bo</td>\n",
       "      <td>Limca Book of World Records - Renu Sukheja</td>\n",
       "      <td>1073</td>\n",
       "      <td>149466</td>\n",
       "      <td>@Neeharika-M</td>\n",
       "      <td>u started a channel ten years agoomg u r so in...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6303 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          videoId                                       title  videoLikes  \\\n",
       "0     Fj-g-cmSzbo                        Beautiful but toxic.       56482   \n",
       "1     Fj-g-cmSzbo                        Beautiful but toxic.       56482   \n",
       "2     Fj-g-cmSzbo                        Beautiful but toxic.       56482   \n",
       "3     Fj-g-cmSzbo                        Beautiful but toxic.       56482   \n",
       "4     Fj-g-cmSzbo                        Beautiful but toxic.       56482   \n",
       "...           ...                                         ...         ...   \n",
       "6298  xxwNglca0Bo  Limca Book of World Records - Renu Sukheja        1073   \n",
       "6299  xxwNglca0Bo  Limca Book of World Records - Renu Sukheja        1073   \n",
       "6300  xxwNglca0Bo  Limca Book of World Records - Renu Sukheja        1073   \n",
       "6301  xxwNglca0Bo  Limca Book of World Records - Renu Sukheja        1073   \n",
       "6302  xxwNglca0Bo  Limca Book of World Records - Renu Sukheja        1073   \n",
       "\n",
       "      videoViews        commenter_name  \\\n",
       "0         808475         @RohitandKanu   \n",
       "1         808475       @rignudignu7200   \n",
       "2         808475          @mastmogachi   \n",
       "3         808475             @DD-pe3no   \n",
       "4         808475              @MrKamei   \n",
       "...          ...                   ...   \n",
       "6298      149466     @dmcindianfan5374   \n",
       "6299      149466       @coolbreeze1262   \n",
       "6300      149466       @Notebook-ur1st   \n",
       "6301      149466  @Prathikraj_mysorean   \n",
       "6302      149466          @Neeharika-M   \n",
       "\n",
       "                                               comments  \n",
       "0     you guys know of any other beautiful but toxic...  \n",
       "1                                             a porsche  \n",
       "2     monkey shoulder  is looking very beautiful rig...  \n",
       "3                                                   NaN  \n",
       "4                                              my crush  \n",
       "...                                                 ...  \n",
       "6298                       just went back 10 years ago   \n",
       "6299                       is the record still to her    \n",
       "6300  wow she looks different  \\ni follow them for e...  \n",
       "6301                        one can see rohits journey   \n",
       "6302  u started a channel ten years agoomg u r so in...  \n",
       "\n",
       "[6303 rows x 6 columns]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.read_csv(\"Data\\\\commentsCleanData.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['comments'] = df['comments'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       [you, guys, know, of, any, other, beautiful, b...\n",
       "1                                            [a, porsche]\n",
       "2       [monkey, shoulder, is, looking, very, beautifu...\n",
       "3                                                   [nan]\n",
       "4                                             [my, crush]\n",
       "                              ...                        \n",
       "6298                   [just, went, back, 10, years, ago]\n",
       "6299                    [is, the, record, still, to, her]\n",
       "6300    [wow, she, looks, different, i, follow, them, ...\n",
       "6301                     [one, can, see, rohits, journey]\n",
       "6302    [u, started, a, channel, ten, years, agoomg, u...\n",
       "Name: tokenized_comments, Length: 6303, dtype: object"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tokenize_text(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    return tokens\n",
    "\n",
    "df['tokenized_comments'] = df['comments'].apply(tokenize_text)\n",
    "df['tokenized_comments']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       [guys, know, beautiful, toxic, things, subscri...\n",
       "1                                               [porsche]\n",
       "2       [monkey, shoulder, looking, beautiful, right, ...\n",
       "3                                                   [nan]\n",
       "4                                                 [crush]\n",
       "                              ...                        \n",
       "6298                         [went, back, 10, years, ago]\n",
       "6299                                      [record, still]\n",
       "6300    [wow, looks, different, follow, english, under...\n",
       "6301                          [one, see, rohits, journey]\n",
       "6302    [u, started, channel, ten, years, agoomg, u, r...\n",
       "Name: tokens_without_stopwords, Length: 6303, dtype: object"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def remove_stopwords(tokens):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_tokens = [token for token in tokens if token.lower() not in stop_words]\n",
    "    return filtered_tokens\n",
    "\n",
    "# Example usage\n",
    "df['tokens_without_stopwords'] = df['tokenized_comments'].apply(remove_stopwords)\n",
    "df['tokens_without_stopwords']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       {'guys': 2, 'know': 1, 'beautiful': 1, 'toxic'...\n",
       "1                                          {'porsche': 1}\n",
       "2       {'monkey': 1, 'shoulder': 1, 'looking': 1, 'be...\n",
       "3                                              {'nan': 1}\n",
       "4                                            {'crush': 1}\n",
       "                              ...                        \n",
       "6298    {'went': 1, 'back': 1, '10': 1, 'years': 1, 'a...\n",
       "6299                            {'record': 1, 'still': 1}\n",
       "6300    {'wow': 1, 'looks': 1, 'different': 1, 'follow...\n",
       "6301      {'one': 1, 'see': 1, 'rohits': 1, 'journey': 1}\n",
       "6302    {'u': 2, 'started': 1, 'channel': 1, 'ten': 1,...\n",
       "Name: token_counts, Length: 6303, dtype: object"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def counter(tokens):\n",
    "    return Counter(tokens)\n",
    "\n",
    "df['token_counts'] = df['tokens_without_stopwords'].apply(lambda x: counter(x))\n",
    "df['token_counts']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       [2, 1, 1, 1, 1, 1, 1, 1, 1]\n",
       "1                               [1]\n",
       "2             [1, 1, 1, 1, 1, 1, 1]\n",
       "3                               [1]\n",
       "4                               [1]\n",
       "                   ...             \n",
       "6298                [1, 1, 1, 1, 1]\n",
       "6299                         [1, 1]\n",
       "6300    [1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
       "6301                   [1, 1, 1, 1]\n",
       "6302       [2, 1, 1, 1, 1, 1, 1, 1]\n",
       "Name: sentance_vector, Length: 6303, dtype: object"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def extract_numbers(dictionary):\n",
    "    return [value for value in dictionary.values()]\n",
    "\n",
    "df['sentance_vector'] = df['token_counts'].apply(extract_numbers)\n",
    "df['sentance_vector']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>videoId</th>\n",
       "      <th>title</th>\n",
       "      <th>videoLikes</th>\n",
       "      <th>videoViews</th>\n",
       "      <th>commenter_name</th>\n",
       "      <th>comments</th>\n",
       "      <th>tokenized_comments</th>\n",
       "      <th>tokens_without_stopwords</th>\n",
       "      <th>token_counts</th>\n",
       "      <th>sentance_vector</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Fj-g-cmSzbo</td>\n",
       "      <td>Beautiful but toxic.</td>\n",
       "      <td>56482</td>\n",
       "      <td>808475</td>\n",
       "      <td>@RohitandKanu</td>\n",
       "      <td>you guys know of any other beautiful but toxic...</td>\n",
       "      <td>[you, guys, know, of, any, other, beautiful, b...</td>\n",
       "      <td>[guys, know, beautiful, toxic, things, subscri...</td>\n",
       "      <td>{'guys': 2, 'know': 1, 'beautiful': 1, 'toxic'...</td>\n",
       "      <td>[2, 1, 1, 1, 1, 1, 1, 1, 1]</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Fj-g-cmSzbo</td>\n",
       "      <td>Beautiful but toxic.</td>\n",
       "      <td>56482</td>\n",
       "      <td>808475</td>\n",
       "      <td>@rignudignu7200</td>\n",
       "      <td>a porsche</td>\n",
       "      <td>[a, porsche]</td>\n",
       "      <td>[porsche]</td>\n",
       "      <td>{'porsche': 1}</td>\n",
       "      <td>[1]</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Fj-g-cmSzbo</td>\n",
       "      <td>Beautiful but toxic.</td>\n",
       "      <td>56482</td>\n",
       "      <td>808475</td>\n",
       "      <td>@mastmogachi</td>\n",
       "      <td>monkey shoulder  is looking very beautiful rig...</td>\n",
       "      <td>[monkey, shoulder, is, looking, very, beautifu...</td>\n",
       "      <td>[monkey, shoulder, looking, beautiful, right, ...</td>\n",
       "      <td>{'monkey': 1, 'shoulder': 1, 'looking': 1, 'be...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1]</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Fj-g-cmSzbo</td>\n",
       "      <td>Beautiful but toxic.</td>\n",
       "      <td>56482</td>\n",
       "      <td>808475</td>\n",
       "      <td>@DD-pe3no</td>\n",
       "      <td>nan</td>\n",
       "      <td>[nan]</td>\n",
       "      <td>[nan]</td>\n",
       "      <td>{'nan': 1}</td>\n",
       "      <td>[1]</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Fj-g-cmSzbo</td>\n",
       "      <td>Beautiful but toxic.</td>\n",
       "      <td>56482</td>\n",
       "      <td>808475</td>\n",
       "      <td>@MrKamei</td>\n",
       "      <td>my crush</td>\n",
       "      <td>[my, crush]</td>\n",
       "      <td>[crush]</td>\n",
       "      <td>{'crush': 1}</td>\n",
       "      <td>[1]</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6298</th>\n",
       "      <td>xxwNglca0Bo</td>\n",
       "      <td>Limca Book of World Records - Renu Sukheja</td>\n",
       "      <td>1073</td>\n",
       "      <td>149466</td>\n",
       "      <td>@dmcindianfan5374</td>\n",
       "      <td>just went back 10 years ago</td>\n",
       "      <td>[just, went, back, 10, years, ago]</td>\n",
       "      <td>[went, back, 10, years, ago]</td>\n",
       "      <td>{'went': 1, 'back': 1, '10': 1, 'years': 1, 'a...</td>\n",
       "      <td>[1, 1, 1, 1, 1]</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6299</th>\n",
       "      <td>xxwNglca0Bo</td>\n",
       "      <td>Limca Book of World Records - Renu Sukheja</td>\n",
       "      <td>1073</td>\n",
       "      <td>149466</td>\n",
       "      <td>@coolbreeze1262</td>\n",
       "      <td>is the record still to her</td>\n",
       "      <td>[is, the, record, still, to, her]</td>\n",
       "      <td>[record, still]</td>\n",
       "      <td>{'record': 1, 'still': 1}</td>\n",
       "      <td>[1, 1]</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6300</th>\n",
       "      <td>xxwNglca0Bo</td>\n",
       "      <td>Limca Book of World Records - Renu Sukheja</td>\n",
       "      <td>1073</td>\n",
       "      <td>149466</td>\n",
       "      <td>@Notebook-ur1st</td>\n",
       "      <td>wow she looks different  \\ni follow them for e...</td>\n",
       "      <td>[wow, she, looks, different, i, follow, them, ...</td>\n",
       "      <td>[wow, looks, different, follow, english, under...</td>\n",
       "      <td>{'wow': 1, 'looks': 1, 'different': 1, 'follow...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1]</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6301</th>\n",
       "      <td>xxwNglca0Bo</td>\n",
       "      <td>Limca Book of World Records - Renu Sukheja</td>\n",
       "      <td>1073</td>\n",
       "      <td>149466</td>\n",
       "      <td>@Prathikraj_mysorean</td>\n",
       "      <td>one can see rohits journey</td>\n",
       "      <td>[one, can, see, rohits, journey]</td>\n",
       "      <td>[one, see, rohits, journey]</td>\n",
       "      <td>{'one': 1, 'see': 1, 'rohits': 1, 'journey': 1}</td>\n",
       "      <td>[1, 1, 1, 1]</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6302</th>\n",
       "      <td>xxwNglca0Bo</td>\n",
       "      <td>Limca Book of World Records - Renu Sukheja</td>\n",
       "      <td>1073</td>\n",
       "      <td>149466</td>\n",
       "      <td>@Neeharika-M</td>\n",
       "      <td>u started a channel ten years agoomg u r so in...</td>\n",
       "      <td>[u, started, a, channel, ten, years, agoomg, u...</td>\n",
       "      <td>[u, started, channel, ten, years, agoomg, u, r...</td>\n",
       "      <td>{'u': 2, 'started': 1, 'channel': 1, 'ten': 1,...</td>\n",
       "      <td>[2, 1, 1, 1, 1, 1, 1, 1]</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6303 rows Ã— 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          videoId                                       title  videoLikes  \\\n",
       "0     Fj-g-cmSzbo                        Beautiful but toxic.       56482   \n",
       "1     Fj-g-cmSzbo                        Beautiful but toxic.       56482   \n",
       "2     Fj-g-cmSzbo                        Beautiful but toxic.       56482   \n",
       "3     Fj-g-cmSzbo                        Beautiful but toxic.       56482   \n",
       "4     Fj-g-cmSzbo                        Beautiful but toxic.       56482   \n",
       "...           ...                                         ...         ...   \n",
       "6298  xxwNglca0Bo  Limca Book of World Records - Renu Sukheja        1073   \n",
       "6299  xxwNglca0Bo  Limca Book of World Records - Renu Sukheja        1073   \n",
       "6300  xxwNglca0Bo  Limca Book of World Records - Renu Sukheja        1073   \n",
       "6301  xxwNglca0Bo  Limca Book of World Records - Renu Sukheja        1073   \n",
       "6302  xxwNglca0Bo  Limca Book of World Records - Renu Sukheja        1073   \n",
       "\n",
       "      videoViews        commenter_name  \\\n",
       "0         808475         @RohitandKanu   \n",
       "1         808475       @rignudignu7200   \n",
       "2         808475          @mastmogachi   \n",
       "3         808475             @DD-pe3no   \n",
       "4         808475              @MrKamei   \n",
       "...          ...                   ...   \n",
       "6298      149466     @dmcindianfan5374   \n",
       "6299      149466       @coolbreeze1262   \n",
       "6300      149466       @Notebook-ur1st   \n",
       "6301      149466  @Prathikraj_mysorean   \n",
       "6302      149466          @Neeharika-M   \n",
       "\n",
       "                                               comments  \\\n",
       "0     you guys know of any other beautiful but toxic...   \n",
       "1                                             a porsche   \n",
       "2     monkey shoulder  is looking very beautiful rig...   \n",
       "3                                                   nan   \n",
       "4                                              my crush   \n",
       "...                                                 ...   \n",
       "6298                       just went back 10 years ago    \n",
       "6299                       is the record still to her     \n",
       "6300  wow she looks different  \\ni follow them for e...   \n",
       "6301                        one can see rohits journey    \n",
       "6302  u started a channel ten years agoomg u r so in...   \n",
       "\n",
       "                                     tokenized_comments  \\\n",
       "0     [you, guys, know, of, any, other, beautiful, b...   \n",
       "1                                          [a, porsche]   \n",
       "2     [monkey, shoulder, is, looking, very, beautifu...   \n",
       "3                                                 [nan]   \n",
       "4                                           [my, crush]   \n",
       "...                                                 ...   \n",
       "6298                 [just, went, back, 10, years, ago]   \n",
       "6299                  [is, the, record, still, to, her]   \n",
       "6300  [wow, she, looks, different, i, follow, them, ...   \n",
       "6301                   [one, can, see, rohits, journey]   \n",
       "6302  [u, started, a, channel, ten, years, agoomg, u...   \n",
       "\n",
       "                               tokens_without_stopwords  \\\n",
       "0     [guys, know, beautiful, toxic, things, subscri...   \n",
       "1                                             [porsche]   \n",
       "2     [monkey, shoulder, looking, beautiful, right, ...   \n",
       "3                                                 [nan]   \n",
       "4                                               [crush]   \n",
       "...                                                 ...   \n",
       "6298                       [went, back, 10, years, ago]   \n",
       "6299                                    [record, still]   \n",
       "6300  [wow, looks, different, follow, english, under...   \n",
       "6301                        [one, see, rohits, journey]   \n",
       "6302  [u, started, channel, ten, years, agoomg, u, r...   \n",
       "\n",
       "                                           token_counts  \\\n",
       "0     {'guys': 2, 'know': 1, 'beautiful': 1, 'toxic'...   \n",
       "1                                        {'porsche': 1}   \n",
       "2     {'monkey': 1, 'shoulder': 1, 'looking': 1, 'be...   \n",
       "3                                            {'nan': 1}   \n",
       "4                                          {'crush': 1}   \n",
       "...                                                 ...   \n",
       "6298  {'went': 1, 'back': 1, '10': 1, 'years': 1, 'a...   \n",
       "6299                          {'record': 1, 'still': 1}   \n",
       "6300  {'wow': 1, 'looks': 1, 'different': 1, 'follow...   \n",
       "6301    {'one': 1, 'see': 1, 'rohits': 1, 'journey': 1}   \n",
       "6302  {'u': 2, 'started': 1, 'channel': 1, 'ten': 1,...   \n",
       "\n",
       "                  sentance_vector sentiment  \n",
       "0     [2, 1, 1, 1, 1, 1, 1, 1, 1]  Positive  \n",
       "1                             [1]   Neutral  \n",
       "2           [1, 1, 1, 1, 1, 1, 1]  Positive  \n",
       "3                             [1]   Neutral  \n",
       "4                             [1]  Negative  \n",
       "...                           ...       ...  \n",
       "6298              [1, 1, 1, 1, 1]   Neutral  \n",
       "6299                       [1, 1]   Neutral  \n",
       "6300  [1, 1, 1, 1, 1, 1, 1, 1, 1]  Positive  \n",
       "6301                 [1, 1, 1, 1]   Neutral  \n",
       "6302     [2, 1, 1, 1, 1, 1, 1, 1]   Neutral  \n",
       "\n",
       "[6303 rows x 11 columns]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def analyze_sentiment(comment):\n",
    "    sid = SentimentIntensityAnalyzer()\n",
    "    sentiment_score = sid.polarity_scores(comment)\n",
    "    if sentiment_score['compound'] >= 0.05:\n",
    "        return 'Positive'\n",
    "    elif sentiment_score['compound'] <= -0.05:\n",
    "        return 'Negative'\n",
    "    else:\n",
    "        return 'Neutral'\n",
    "\n",
    "df['sentiment'] = df['comments'].apply(analyze_sentiment)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 1: cheers, subscribe, enjoying, share, youre, content, scam, bloody, like, learn\n",
      "Topic 2: love, videos, lots, way, petrol, lanka, sri, goa, tamilnadu, pune\n",
      "Topic 3: bro, phone, moves, mast, joke, come, superb, exact, sis, channel\n",
      "Topic 4: like, sounds, couples, think, prabhas, doesnt, wish, single, baby, feel\n",
      "Topic 5: guys, congratulations, acting, english, happy, hilarious, hope, nailed, fun, watching\n",
      "Topic 6: rohit, kanupriya, sister, handsome, congratulations, prabhas, poor, proud, bhai, food\n",
      "Topic 7: dont, understand, worry, fall, want, make, money, rohitandkanu, ro, say\n",
      "Topic 8: kanu, expressions, ad, sexy, mayonnaise, acted, poor, cutest, life, lovely\n",
      "Topic 9: nice, word, try, place, haha, humour, hyderabad, dude, couples, question\n",
      "Topic 10: good, acting, looking, actor, boy, bad, way, script, hahaha, soo\n",
      "Topic 11: thats, yeah, started, real, totally, joke, feel, hey, cold, deep\n",
      "Topic 12: best, friend, hilarious, channel, seen, day, joke, wow, hahaha, couples\n",
      "Topic 13: man, lucky, real, camera, sweet, oscar, killed, handsome, dead, mean\n",
      "Topic 14: just, want, make, stop, saw, say, brilliant, day, feel, kidding\n",
      "Topic 15: cute, girl, priya, soo, smile, reality, wish, hyderabad, arent, voice\n",
      "Topic 16: video, make, waiting, thing, stupid, watching, thank, seen, pls, missed\n",
      "Topic 17: true, damn, 100, haha, soooo, dude, happened, underrated, fuck, absolutely\n",
      "Topic 18: im, glad, brother, cancer, fine, gonna, try, exactly, dead, day\n",
      "Topic 19: understand, didnt, explain, tell, expect, people, mean, said, times, fr\n",
      "Topic 20: lol, expression, exactly, say, face, use, yup, hand, behave, worli\n",
      "Topic 21: relatable, husband, totally, 100, sooo, soo, quite, damn, genuine, freaking\n",
      "Topic 22: ur, videos, watching, sister, hi, watch, brother, mom, channel, loving\n",
      "Topic 23: couple, lovely, sweet, fav, crazy, bless, handsome, youtube, seen, relate\n",
      "Topic 24: hai, bhai, kya, hi, bhi, ho, ye, ka, ko, nahi\n",
      "Topic 25: time, happens, long, waste, thing, english, relate, took, closed, today\n",
      "Topic 26: awesome, expressions, thanks, word, laugh, share, editing, punch, boobs, rohith\n",
      "Topic 27: amazing, waiting, laughing, dance, said, yr, music, rofl, stop, skills\n",
      "Topic 28: really, pretty, nailed, lot, wish, jokes, lucky, happened, working, enjoyed\n",
      "Topic 29: wife, husband, sister, said, ask, tell, thank, think, second, sexy\n",
      "Topic 30: beautiful, girl, madam, mom, sis, place, omg, hot, smile, wow\n",
      "Topic 31: twist, plot, unexpected, waiting, having, haha, knew, twists, expected, helpline\n",
      "Topic 32: funny, haha, day, dance, actually, going, sooo, yaar, extremely, watched\n",
      "Topic 33: did, said, come, say, tell, bruh, babe, hell, thing, means\n",
      "Topic 34: god, bless, oh, real, damn, thank, son, sweet, lovely, dance\n",
      "Topic 35: men, women, need, life, think, boys, things, real, way, half\n",
      "Topic 36: got, lucky, happy, expression, bruh, word, new, married, laughing, dying\n",
      "Topic 37: know, let, want, girl, means, guess, sir, phone, wanna, hey\n",
      "Topic 38: yes, say, baby, happened, sir, said, thank, rohith, opposite, says\n",
      "Topic 39: end, laugh, smile, expression, said, tho, hilarious, way, guess, hungry\n",
      "Topic 40: look, prabhas, exactly, handsome, mean, sister, omg, indian, actor, hug\n",
      "Topic 41: hindi, english, make, bol, speak, bhi, learn, talk, karo, language\n",
      "Topic 42: guy, hot, bad, smart, says, lucky, shes, day, called, waiting\n",
      "Topic 43: looks, sister, soo, old, prabhas, indian, south, sir, future, model\n",
      "Topic 44: super, pair, nailed, excited, cringe, ro, season, ha, agree, oh\n",
      "Topic 45: thought, people, say, money, face, going, youre, gonna, oh, better\n",
      "Topic 46: perfect, does, need, word, transition, smile, ass, pair, leg, example\n",
      "Topic 47: right, hes, damn, actually, way, mean, said, whats, comes, answer\n",
      "Topic 48: great, hilarious, shes, welcome, doing, brother, channel, making, answer, tamil\n",
      "Topic 49: song, whats, music, tell, pls, played, knows, called, background, plz\n",
      "Topic 50: content, level, brother, acting, make, channel, original, making, enjoying, boring\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import NMF\n",
    "\n",
    "def nmf_topic_modeling(comments, num_topics=50):\n",
    "    # Create TF-IDF matrix\n",
    "    tfidf_vectorizer = TfidfVectorizer(max_df=0.95, min_df=2, stop_words='english')\n",
    "    tfidf = tfidf_vectorizer.fit_transform(comments)\n",
    "    \n",
    "    # Apply NMF\n",
    "    nmf_model = NMF(n_components=num_topics, random_state=42)\n",
    "    nmf_model.fit(tfidf)\n",
    "    \n",
    "    # Get the top words for each topic\n",
    "    feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "    topic_words = []\n",
    "    for topic_idx, topic in enumerate(nmf_model.components_):\n",
    "        top_words_idx = topic.argsort()[:-10 - 1:-1]\n",
    "        top_words = [feature_names[i] for i in top_words_idx]\n",
    "        topic_words.append(top_words)\n",
    "    \n",
    "    return topic_words\n",
    "\n",
    "# Assuming df_cleaned is your DataFrame with cleaned comments\n",
    "# Replace this with your actual DataFrame\n",
    "comments = df['comments']\n",
    "\n",
    "# Perform topic modeling using NMF\n",
    "topics = nmf_topic_modeling(comments)\n",
    "\n",
    "# Print the top words for each topic\n",
    "for idx, topic in enumerate(topics):\n",
    "    print(f\"Topic {idx+1}: {', '.join(topic)}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
